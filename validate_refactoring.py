"""
Validation Script

Script para validar que la refactorizaciÃ³n del mÃ³dulo agent estÃ© completa y funcional.
"""

import os
import sys
import importlib
from pathlib import Path
from typing import List, Dict, Any

def validate_structure() -> Dict[str, Any]:
    """Validar la estructura de directorios refactorizada"""
    print("ğŸ” Validando estructura de directorios...")
    
    expected_structure = {
        "agent/__init__.py": "Punto de entrada principal",
        "agent/core/__init__.py": "MÃ³dulo core",
        "agent/core/orchestrator.py": "Orquestador principal",
        "agent/core/query_classifier.py": "Clasificador de consultas",
        "agent/core/response_evaluator.py": "Evaluador de respuestas",
        "agent/specialists/__init__.py": "Agentes especializados",
        "agent/specialists/clarifier.py": "Agente clarificador",
        "agent/specialists/email_handler.py": "Agente de email",
        "agent/utils/__init__.py": "Utilidades",
        "agent/utils/config.py": "ConfiguraciÃ³n",
        "agent/utils/logger.py": "Sistema de logging",
        "agent/utils/prompts.py": "Gestor de prompts",
        "agent/README.md": "DocumentaciÃ³n"
    }
    
    results = {
        "missing_files": [],
        "existing_files": [],
        "total_expected": len(expected_structure),
        "total_found": 0
    }
    
    for file_path, description in expected_structure.items():
        if Path(file_path).exists():
            results["existing_files"].append(file_path)
            results["total_found"] += 1
            print(f"   âœ… {file_path} - {description}")
        else:
            results["missing_files"].append(file_path)
            print(f"   âŒ {file_path} - {description} (MISSING)")
    
    coverage = (results["total_found"] / results["total_expected"]) * 100
    print(f"\nğŸ“Š Cobertura de estructura: {coverage:.1f}% ({results['total_found']}/{results['total_expected']})")
    
    return results

def validate_imports() -> Dict[str, Any]:
    """Validar que las importaciones funcionen correctamente"""
    print("\nğŸ”— Validando importaciones...")
    
    import_tests = [
        ("agent", "MÃ³dulo principal"),
        ("agent.core.orchestrator", "CVOrchestrator"),
        ("agent.core.query_classifier", "QueryClassifier y QueryClassification"),
        ("agent.core.response_evaluator", "ResponseEvaluator y EvaluationResult"),
        ("agent.specialists.clarifier", "ClarifierAgent"),
        ("agent.specialists.email_handler", "EmailAgent"),
        ("agent.utils.config", "AgentConfig"),
        ("agent.utils.logger", "AgentLogger"),
        ("agent.utils.prompts", "PromptManager")
    ]
    
    results = {
        "successful_imports": [],
        "failed_imports": [],
        "total_tests": len(import_tests),
        "total_passed": 0
    }
    
    for module_name, description in import_tests:
        try:
            importlib.import_module(module_name)
            results["successful_imports"].append(module_name)
            results["total_passed"] += 1
            print(f"   âœ… {module_name} - {description}")
        except ImportError as e:
            results["failed_imports"].append((module_name, str(e)))
            print(f"   âŒ {module_name} - {description} (ERROR: {e})")
        except Exception as e:
            results["failed_imports"].append((module_name, str(e)))
            print(f"   âš ï¸  {module_name} - {description} (WARNING: {e})")
    
    success_rate = (results["total_passed"] / results["total_tests"]) * 100
    print(f"\nğŸ“Š Tasa de Ã©xito de importaciones: {success_rate:.1f}% ({results['total_passed']}/{results['total_tests']})")
    
    return results

def validate_classes() -> Dict[str, Any]:
    """Validar que las clases principales se puedan instanciar"""
    print("\nğŸ—ï¸ Validando clases principales...")
    
    results = {
        "successful_classes": [],
        "failed_classes": [],
        "warnings": []
    }
    
    # Test AgentConfig
    try:
        from agent.utils.config import AgentConfig
        config = AgentConfig.from_env()
        results["successful_classes"].append("AgentConfig")
        print("   âœ… AgentConfig - ConfiguraciÃ³n cargada correctamente")
    except ValueError as e:
        results["warnings"].append(f"AgentConfig: {e}")
        print(f"   âš ï¸  AgentConfig - {e}")
    except Exception as e:
        results["failed_classes"].append(f"AgentConfig: {e}")
        print(f"   âŒ AgentConfig - ERROR: {e}")
        return results  # No podemos continuar sin configuraciÃ³n
    
    # Test AgentLogger
    try:
        from agent.utils.logger import AgentLogger
        logger = AgentLogger("test")
        results["successful_classes"].append("AgentLogger")
        print("   âœ… AgentLogger - Logger inicializado correctamente")
    except Exception as e:
        results["failed_classes"].append(f"AgentLogger: {e}")
        print(f"   âŒ AgentLogger - ERROR: {e}")
    
    # Test PromptManager
    try:
        from agent.utils.prompts import PromptManager
        prompt_manager = PromptManager()
        results["successful_classes"].append("PromptManager")
        print("   âœ… PromptManager - Gestor de prompts inicializado correctamente")
    except Exception as e:
        results["failed_classes"].append(f"PromptManager: {e}")
        print(f"   âŒ PromptManager - ERROR: {e}")
    
    # Test QueryClassifier (requiere OpenAI)
    try:
        from agent.core.query_classifier import QueryClassifier
        classifier = QueryClassifier(config)
        results["successful_classes"].append("QueryClassifier")
        print("   âœ… QueryClassifier - Clasificador inicializado correctamente")
    except Exception as e:
        results["warnings"].append(f"QueryClassifier: {e}")
        print(f"   âš ï¸  QueryClassifier - {e}")
    
    # Test ResponseEvaluator (requiere OpenAI)
    try:
        from agent.core.response_evaluator import ResponseEvaluator
        evaluator = ResponseEvaluator(config)
        results["successful_classes"].append("ResponseEvaluator")
        print("   âœ… ResponseEvaluator - Evaluador inicializado correctamente")
    except Exception as e:
        results["warnings"].append(f"ResponseEvaluator: {e}")
        print(f"   âš ï¸  ResponseEvaluator - {e}")
    
    # Test ClarifierAgent (requiere OpenAI)
    try:
        from agent.specialists.clarifier import ClarifierAgent
        clarifier = ClarifierAgent(config)
        results["successful_classes"].append("ClarifierAgent")
        print("   âœ… ClarifierAgent - Agente clarificador inicializado correctamente")
    except Exception as e:
        results["warnings"].append(f"ClarifierAgent: {e}")
        print(f"   âš ï¸  ClarifierAgent - {e}")
    
    # Test EmailAgent
    try:
        from agent.specialists.email_handler import EmailAgent
        email_agent = EmailAgent(config)
        results["successful_classes"].append("EmailAgent")
        print("   âœ… EmailAgent - Agente de email inicializado correctamente")
        
        if not email_agent.is_configured:
            results["warnings"].append("EmailAgent: Email no configurado completamente")
            print("   âš ï¸  EmailAgent - Email no configurado completamente (opcional)")
    except Exception as e:
        results["failed_classes"].append(f"EmailAgent: {e}")
        print(f"   âŒ EmailAgent - ERROR: {e}")
    
    # Test CVOrchestrator (integraciÃ³n completa)
    try:
        from agent.core.orchestrator import CVOrchestrator
        orchestrator = CVOrchestrator(config)
        results["successful_classes"].append("CVOrchestrator")
        print("   âœ… CVOrchestrator - Orquestador principal inicializado correctamente")
    except Exception as e:
        results["failed_classes"].append(f"CVOrchestrator: {e}")
        print(f"   âŒ CVOrchestrator - ERROR: {e}")
    
    print(f"\nğŸ“Š Clases validadas exitosamente: {len(results['successful_classes'])}")
    print(f"ğŸ“Š Advertencias: {len(results['warnings'])}")
    print(f"ğŸ“Š Errores: {len(results['failed_classes'])}")
    
    return results

def validate_functionality() -> Dict[str, Any]:
    """Validar funcionalidad bÃ¡sica del sistema"""
    print("\nâš™ï¸ Validando funcionalidad bÃ¡sica...")
    
    results = {
        "functional_tests": [],
        "failed_tests": [],
        "warnings": []
    }
    
    try:
        from agent.utils.config import AgentConfig
        from agent.core.orchestrator import CVOrchestrator
        
        config = AgentConfig.from_env()
        orchestrator = CVOrchestrator(config)
        
        # Test clasificaciÃ³n bÃ¡sica
        try:
            classification = orchestrator.query_classifier.classify("Â¿CuÃ¡l es tu email?")
            if hasattr(classification, 'category') and hasattr(classification, 'confidence'):
                results["functional_tests"].append("Query classification")
                print("   âœ… ClasificaciÃ³n de consultas - Funcional")
            else:
                results["failed_tests"].append("Query classification: Invalid response structure")
                print("   âŒ ClasificaciÃ³n de consultas - Estructura de respuesta invÃ¡lida")
        except Exception as e:
            results["warnings"].append(f"Query classification: {e}")
            print(f"   âš ï¸  ClasificaciÃ³n de consultas - {e}")
        
        # Test evaluaciÃ³n bÃ¡sica
        try:
            evaluation = orchestrator.response_evaluator.evaluate_response(
                query="Test query",
                response="Test response",
                context="Test context"
            )
            if hasattr(evaluation, 'scores') and hasattr(evaluation, 'confidence'):
                results["functional_tests"].append("Response evaluation")
                print("   âœ… EvaluaciÃ³n de respuestas - Funcional")
            else:
                results["failed_tests"].append("Response evaluation: Invalid response structure")
                print("   âŒ EvaluaciÃ³n de respuestas - Estructura de respuesta invÃ¡lida")
        except Exception as e:
            results["warnings"].append(f"Response evaluation: {e}")
            print(f"   âš ï¸  EvaluaciÃ³n de respuestas - {e}")
        
        # Test generaciÃ³n de aclaraciones
        try:
            questions = orchestrator.clarifier_agent.generate_clarifications("Test query")
            if isinstance(questions, list) and len(questions) > 0:
                results["functional_tests"].append("Clarification generation")
                print("   âœ… GeneraciÃ³n de aclaraciones - Funcional")
            else:
                results["failed_tests"].append("Clarification generation: No questions generated")
                print("   âŒ GeneraciÃ³n de aclaraciones - No se generaron preguntas")
        except Exception as e:
            results["warnings"].append(f"Clarification generation: {e}")
            print(f"   âš ï¸  GeneraciÃ³n de aclaraciones - {e}")
        
        # Test configuraciÃ³n de email
        try:
            email_test = orchestrator.email_agent.test_email_configuration()
            if email_test["configuration_valid"]:
                results["functional_tests"].append("Email configuration")
                print("   âœ… ConfiguraciÃ³n de email - VÃ¡lida")
            else:
                results["warnings"].append("Email configuration: Not configured")
                print("   âš ï¸  ConfiguraciÃ³n de email - No configurada (opcional)")
        except Exception as e:
            results["warnings"].append(f"Email configuration: {e}")
            print(f"   âš ï¸  ConfiguraciÃ³n de email - {e}")
        
        # Test procesamiento completo de consulta
        try:
            result = orchestrator.process_query("Â¿CuÃ¡l es tu experiencia?")
            if "response" in result and "classification" in result:
                results["functional_tests"].append("Full query processing")
                print("   âœ… Procesamiento completo de consultas - Funcional")
            else:
                results["failed_tests"].append("Full query processing: Invalid response structure")
                print("   âŒ Procesamiento completo de consultas - Estructura de respuesta invÃ¡lida")
        except Exception as e:
            results["warnings"].append(f"Full query processing: {e}")
            print(f"   âš ï¸  Procesamiento completo de consultas - {e}")
        
    except Exception as e:
        results["failed_tests"].append(f"Functionality validation setup: {e}")
        print(f"   âŒ Error configurando validaciÃ³n de funcionalidad: {e}")
    
    print(f"\nğŸ“Š Tests funcionales exitosos: {len(results['functional_tests'])}")
    print(f"ğŸ“Š Advertencias: {len(results['warnings'])}")
    print(f"ğŸ“Š Errores: {len(results['failed_tests'])}")
    
    return results

def generate_report(structure_results: Dict, import_results: Dict, 
                   class_results: Dict, functionality_results: Dict):
    """Generar reporte final de validaciÃ³n"""
    print("\n" + "="*60)
    print("ğŸ“‹ REPORTE FINAL DE VALIDACIÃ“N")
    print("="*60)
    
    # Resumen general
    total_issues = (
        len(structure_results["missing_files"]) +
        len(import_results["failed_imports"]) +
        len(class_results["failed_classes"]) +
        len(functionality_results["failed_tests"])
    )
    
    total_warnings = (
        len(class_results["warnings"]) +
        len(functionality_results["warnings"])
    )
    
    if total_issues == 0:
        print("ğŸ‰ Â¡VALIDACIÃ“N EXITOSA! La refactorizaciÃ³n estÃ¡ completa.")
        status = "âœ… EXITOSO"
    elif total_issues <= 2:
        print("âš ï¸  VALIDACIÃ“N MAYORMENTE EXITOSA con problemas menores.")
        status = "âš ï¸  PARCIAL"
    else:
        print("âŒ VALIDACIÃ“N FALLIDA. Se requieren correcciones.")
        status = "âŒ FALLIDA"
    
    print(f"\nğŸ“Š RESUMEN:")
    print(f"   Estado: {status}")
    print(f"   Errores crÃ­ticos: {total_issues}")
    print(f"   Advertencias: {total_warnings}")
    
    # Detalles por categorÃ­a
    print(f"\nğŸ“ ESTRUCTURA:")
    print(f"   Archivos encontrados: {structure_results['total_found']}/{structure_results['total_expected']}")
    if structure_results["missing_files"]:
        print(f"   Archivos faltantes: {', '.join(structure_results['missing_files'])}")
    
    print(f"\nğŸ”— IMPORTACIONES:")
    print(f"   Importaciones exitosas: {import_results['total_passed']}/{import_results['total_tests']}")
    if import_results["failed_imports"]:
        print(f"   Importaciones fallidas:")
        for module, error in import_results["failed_imports"]:
            print(f"     - {module}: {error}")
    
    print(f"\nğŸ—ï¸  CLASES:")
    print(f"   Clases funcionales: {len(class_results['successful_classes'])}")
    if class_results["failed_classes"]:
        print(f"   Clases con errores:")
        for error in class_results["failed_classes"]:
            print(f"     - {error}")
    if class_results["warnings"]:
        print(f"   Advertencias:")
        for warning in class_results["warnings"]:
            print(f"     - {warning}")
    
    print(f"\nâš™ï¸  FUNCIONALIDAD:")
    print(f"   Tests funcionales exitosos: {len(functionality_results['functional_tests'])}")
    if functionality_results["failed_tests"]:
        print(f"   Tests fallidos:")
        for error in functionality_results["failed_tests"]:
            print(f"     - {error}")
    if functionality_results["warnings"]:
        print(f"   Advertencias funcionales:")
        for warning in functionality_results["warnings"]:
            print(f"     - {warning}")
    
    # Recomendaciones
    print(f"\nğŸ’¡ RECOMENDACIONES:")
    
    if structure_results["missing_files"]:
        print("   - Crear archivos faltantes de la estructura")
    
    if import_results["failed_imports"]:
        print("   - Revisar y corregir errores de importaciÃ³n")
    
    if total_issues == 0 and total_warnings == 0:
        print("   - Â¡Todo perfecto! El sistema estÃ¡ listo para uso.")
    elif total_issues == 0:
        print("   - Sistema funcional. Considerar resolver advertencias para funcionalidad completa.")
    else:
        print("   - Resolver errores crÃ­ticos antes de usar el sistema en producciÃ³n.")
        print("   - Verificar configuraciÃ³n de variables de entorno (especialmente OPENAI_API_KEY)")
    
    print("\n" + "="*60)
    
    # CÃ³digo de salida
    return 0 if total_issues == 0 else 1

def main():
    """FunciÃ³n principal de validaciÃ³n"""
    print("ğŸ” VALIDADOR DE REFACTORIZACIÃ“N - MÃ³dulo Agent")
    print("="*60)
    
    # Verificar que estamos en el directorio correcto
    if not Path("agent").exists():
        print("âŒ Error: No se encuentra el directorio 'agent'")
        print("ğŸ’¡ Ejecutar desde el directorio raÃ­z del proyecto")
        return 1
    
    try:
        # Ejecutar validaciones
        structure_results = validate_structure()
        import_results = validate_imports()
        class_results = validate_classes()
        functionality_results = validate_functionality()
        
        # Generar reporte final
        return generate_report(structure_results, import_results, 
                             class_results, functionality_results)
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ValidaciÃ³n interrumpida por el usuario")
        return 1
    except Exception as e:
        print(f"\nâŒ Error inesperado durante la validaciÃ³n: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
#!/usr/bin/env python
"""
Quick Start - Multi-LLM

Script de inicio r√°pido para probar el Multi-LLM Client
"""

import os
from dotenv import load_dotenv

load_dotenv()


def show_status():
    """Mostrar estado de configuraci√≥n"""
    print("\n" + "=" * 60)
    print("ü§ñ MULTI-LLM PLUG-AND-PLAY - QUICK START")
    print("=" * 60 + "\n")
    
    # Detectar proveedores configurados
    providers = {
        "OpenAI": os.getenv("OPENAI_API_KEY"),
        "DeepSeek": os.getenv("DEEPSEEK_API_KEY"),
        "Groq": os.getenv("GROQ_API_KEY"),
    }
    
    print("üìã Estado de Configuraci√≥n:\n")
    for provider, key in providers.items():
        status = "‚úÖ Configurado" if key else "‚ùå No configurado"
        print(f"   {provider:12s}: {status}")
    
    print("\n" + "=" * 60)
    print("üìö Recursos Disponibles:")
    print("=" * 60 + "\n")
    
    resources = {
        "Gu√≠a completa": "docs/MULTI_LLM_GUIDE.md",
        "Demo interactiva": "python examples/multi_llm_demo.py",
        "Resumen implementaci√≥n": "IMPLEMENTATION_MULTI_LLM.md",
        "Configuraci√≥n": ".env.example"
    }
    
    for name, path in resources.items():
        print(f"   üìÑ {name:25s}: {path}")
    
    print("\n" + "=" * 60)
    print("üöÄ Uso R√°pido:")
    print("=" * 60 + "\n")
    
    print("""   # 1. Configurar provider en .env
   LLM_PROVIDER=openai
   OPENAI_API_KEY=sk-...
   
   # 2. Usar en c√≥digo
   from agent.utils.multi_llm_client import create_multi_llm_client
   
   client = create_multi_llm_client(provider="openai")
   response = client.generate([
       {"role": "user", "content": "Hello!"}
   ])
   print(response.content)
   
   # 3. O usar con CVOrchestrator
   from agent.core.orchestrator import CVOrchestrator
   
   orchestrator = CVOrchestrator()
   result = orchestrator.process_query("¬øCu√°les son mis proyectos?")
""")
    
    print("\n" + "=" * 60)
    print("üí° Proveedores Soportados:")
    print("=" * 60 + "\n")
    
    providers_info = [
        ("OpenAI", "GPT-4, GPT-3.5", "M√°xima calidad"),
        ("DeepSeek", "deepseek-chat", "Econ√≥mico (~99% m√°s barato)"),
        ("Groq", "Mixtral, Llama", "Ultra r√°pido (gratis)"),
        ("Ollama", "Llama, Mistral", "Local (sin API key)")
    ]
    
    for provider, models, note in providers_info:
        print(f"   üîπ {provider:10s}: {models:20s} - {note}")
    
    print("\n" + "=" * 60)
    print("üìñ Siguiente Paso:")
    print("=" * 60 + "\n")
    
    if not os.getenv("OPENAI_API_KEY"):
        print("   ‚ö†Ô∏è  Configura OPENAI_API_KEY en .env")
        print("   üìù Copia .env.example a .env y a√±ade tu API key\n")
    else:
        print("   ‚úÖ Configuraci√≥n lista!")
        print("   üéØ Ejecuta: python examples/multi_llm_demo.py\n")


def test_basic():
    """Test b√°sico de funcionalidad"""
    print("\n" + "=" * 60)
    print("üß™ Test de Funcionalidad B√°sica")
    print("=" * 60 + "\n")
    
    try:
        from agent.utils.multi_llm_client import create_multi_llm_client
        print("   ‚úÖ Imports exitosos")
        
        # Solo test de creaci√≥n, no llamada real
        api_key = os.getenv("OPENAI_API_KEY", "test-key")
        client = create_multi_llm_client(
            provider="openai",
            api_key=api_key,
            model="gpt-3.5-turbo"
        )
        print("   ‚úÖ Cliente creado correctamente")
        
        print("\n   üéâ Multi-LLM Client est√° funcionando!\n")
        
        if api_key == "test-key":
            print("   ‚ÑπÔ∏è  Configura OPENAI_API_KEY para hacer llamadas reales\n")
        
    except Exception as e:
        print(f"   ‚ùå Error: {e}\n")


if __name__ == "__main__":
    show_status()
    test_basic()
    
    print("=" * 60)
    print("‚ú® Listo para usar Multi-LLM!")
    print("=" * 60 + "\n")
